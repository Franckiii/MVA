\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}

\title{Homework 2 Convex Optimization}
\author{Franck Laborde}
\date{October 2024}

\begin{document}

\maketitle

\section*{Exercice 1 (LP Duality)}
    \begin{itemize}
        \item [1.] Let us compute the Lagrangian of the problem (P). We have : \begin{equation}
            L(x,\lambda, \nu)=c^Tx-\lambda^Tx+\nu^T(b-Ax) = (c-\lambda-A^T\nu)^Tx+\nu^Tb
        \end{equation}
        with $\lambda\in \mathbb{R}^d$ and $\nu\in\mathbb{R}^n$.
        
        To find $g(\lambda, \nu)$ the Lagrange dual function, we need to minimize the Lagrangian in respect to $x$. To do that, the coefficient of $x$ need to vanish, it is to say : $c-\lambda-A^T\nu = 0 \Leftrightarrow \lambda=c-A^T\nu$.
        
        In this case, $L(x,\lambda,\nu)=\nu^Tb = b^T\nu$

        Thus, the dual problem is :
        \begin{equation}
            \begin{aligned}
                &\max_{\nu} \quad b^T\nu \\
                &\text{s.t.} \quad A^T\nu\leq c
            \end{aligned}
        \end{equation}

        \item[2.] The Lagrangian of the problem (D) is : 
        \begin{equation}
            L(y,\lambda) = -b^Ty+\lambda^T(A^Ty-c)=(-b+A\lambda)^Ty-\lambda^Tc
        \end{equation}
        with $\lambda\in\mathbb{R}^d$.

        As in the previous question, the minimum of the Lagrangian in respect to $y$ happend when the coefficient of $y$ vanishes, meaning : $A\lambda=b$. 

        In this case, $L(x, \lambda, \nu)=-\lambda^Tc = -c^T\lambda$

        Thus, the dual problem is given by :
        \begin{equation}
            \begin{aligned}
                &\min_{\lambda} \quad c^T\lambda \\
                &\text{s.t.} \quad A\lambda=b, \\
                & \quad \quad \lambda\geq 0
            \end{aligned}
        \end{equation}
        \item[3.] The Lagrangian of the self-dual problem is : 
        \begin{equation}
            \begin{align}
                L(x,y,\lambda_1, \lambda_2, \nu) & =c^Tx-b^Ty-\lambda_1^Tx+\lambda_2(A^Ty-c)+\nu(b-Ax) \\
                & = (c-\lambda_1-A^T\nu)^Tx+(A\lambda_2-b)^Ty-\lambda_2^Tc+\nu^Tb
            \end{align}
        \end{equation}
        It comes that the Lagrangian is minimal in respect to $x$ and $y$ when both coefficients on $x$ and $y$ vanish, which gives two conditions : $\lambda_1 = c-A^T\nu$ and $A\lambda_2=b$.

        The Lagrange dual function is then :
        \begin{equation}
            g(\lambda, \nu)=\nu^Tb-\lambda^Tc=b^T\nu-c^T\lambda
        \end{equation}

        Thus, the dual problem is :
        \begin{equation}
            \begin{aligned}
                &\max_{\lambda, \nu} \quad b^T\nu-c^T\lambda \\
                &\text{s.t.} \quad \lambda\geq0 \\
                & \quad \quad A\lambda=b\\
                & \quad \lambda_1 \geq 0 \Leftrightarrow A^T\nu \leq c
            \end{aligned}
        \end{equation}
        If we change it for a minimization problem, we have the original problem. This shows that the problem (Self-Dual) is indeed self dual.
        \item[4.]  Assuming that the (Self-Dual) problem is feasible and bounded, let $[x^*, y^*]$ be its optimal solution. The problem (P) respects the Slater's constraint qualification as it is a linear thus convex problem. This implies that the strong duality holds for the problem (P).
        
        This means that both $[x^*, y^*]$ satisfies the constraints of the problem (P) for $x^*$ and the constraints of the problem (D) for $y^*$. Moreover, $x^*$ must be an optimal solution to (P) and $y^*$ to (D) (consequence of the strong duality). This shows that the vector $[x^*, y^*]$ can also be obtained by solving the problems (P) and (D)

        To solve the optimal value, we need to solve 
        \begin{equation}
            \min_{x,y} \quad c^Tx^*-b^Ty^*
        \end{equation}
        but we also have, from the strong duality of the problems (P) and (D) : 
        \begin{equation}
            c^Tx^*=b^Ty^* \Leftrightarrow c^Tx^*-b^Ty^*=0
        \end{equation}
        So the optimal value is exactly 0.
    \end{itemize}

\section*{Exercise 2 (Regularized Least-Square)}
    \begin{itemize}
        \item [1.] We need to compute the conjugate of $||x||_1$, meaning $f^*(y)=\sup_{x\in\mathbb{R}^d}(y^Tx-||x||_1)$. To improve readability, we will note the $l_1$-norm $||.||$ instead of $||.||_1$.

        To compute $f^*$, we will perform a case analysis. Let $y\in\mathbb{R}^d$ : 
        \begin{itemize}
            \item \textbf{Case $||y||_\infty\leq 1$}: As the $l_1$-norm can be expressed as $\max_{||p||<1}x^Tp>x^Ty$. 

            Then :
            \begin{itemize}
                \item $x^Ty-||x||\leq0, \quad \forall x\in\mathbb{R}^d$,
                \item $x^Ty-||x||\leq0, \quad \text{at} \quad x=0$
            \end{itemize}
            Therefore, in this case, $f^*(y)=0$
            \item \textbf{Case $||y||_\infty> 1$}: We can rewrite $x=||x||\frac{x}{||x||} = ||x||x_0$.
            
            Then, $y^Tx-||x||=||x||(y^Tx_0-1)$ with $y^Tx_0-1 > 0$ so the maximum is clearly $+\infty$ as it is maximizing $||x||$ over $\mathbb{R}^d$.
        \end{itemize}
        Thus, the conjugate of $||x||$ is :
        \begin{equation}
            f^*(y)=
            \begin{cases}
                0 \quad \text{if } ||y||_\infty\leq 1,\\
                +\infty \quad\text{otherwise}
            \end{cases}
        \end{equation}
        \item[2.]  Let us begin by noticing that, if we introduce the variable $z=Ax-b$, the optimization problem (RLS) can be rewrite as : 
        \begin{equation}
            \begin{aligned}
                & \min_x \quad ||z||_2^2+||x||_1 \\
                & \text{s.t.} \quad z=Ax-b
            \end{aligned}
        \end{equation}
        In this case, the Lagrangian is :
        \begin{equation}
            L(x,z,\nu)=||z||_2^2+||x||_1+\nu^T(Ax-b-z)
        \end{equation}
        To compute the dual problem, we need to solve two optimization problems : $\min_x(||x||_1+\nu^TAx)$ and $\min_z(||z||_2^2-\nu^Tz)$
        \begin{itemize}
            \item $\min_x(||x||_1+\nu^TAx)$
            
            That problem can be solved by using the conjugate of the $l_1$-norm, found in the previous question. In fact, we have the equivalent problem: $\max_x(-\nu^TAx-||x||_1)$.

            The solution is then equal to 0 when $||\nu^TA||_\infty\leq1$.
            \item $\min_z(||z||_2^2-\nu^Tz)$

            To solve this problem, we can developp the expression of the $l_2$-norm and noticing that it can be expressed as a distance :
            \begin{equation}
                \begin{align}
                    ||z||_2^2-\nu^Tz = &z^Tz-\nu^Tz\\
                    &=\bigg(z-\frac{\nu}{2}\bigg)^T\bigg(z-\frac{\nu}{2}\bigg)-\frac{1}{4}||\nu||_2^2
                \end{align}    
            \end{equation}
            As the first is a distance, it is positive, which means that the solution of the optimization problem on $z$ is reached when $z=0$ and its minimum is $-\frac{1}{4}||\nu||_2^2$
        \end{itemize}
        If we use the two results showed above, the Lagrange dual function is $g(\nu)=-\frac{1}{4}||\nu||_2^2-\nu^Tb$ and the dual problem of (RLS) is 
        \begin{equation}
            \begin{aligned}
                &\max_\nu \quad -\frac{1}{4}||\nu||_2^2-\nu^Tb\\
                &\text{s.t.} \quad ||\nu^TA||_\infty\leq1
            \end{aligned}
        \end{equation}
    \end{itemize}
\section*{Exercise 3 (Data Separation)}
    \begin{itemize}
        \item [1.] First of all, multiplying the problem (Sep 2) by $\tau$ does not change it.

        The new variable introduced $z$ handle the loss function of the problem (Sep 1), specifically since $z\geq0$ and :

        $\forall i, z_i\geq 1-y_i(\omega^Tx_i)$

        We observe that 
        \begin{itemize}
            \item If $1-y_i(\omega^Tx_i)\geq0$, $z_i\geq1-y_i(\omega^Tx_i)$ 
            \item At the contrary, if $1-y_i(\omega^Tx_i)\leq0$, $z_i=0$.
        \end{itemize} 

        Thus, $z_i$ represents the loss term $\max(0;1-y_i(\omega^Tx_i))$ for each data point.
        
        Moreover, as we are summing all the components of $z$, we are minimizing $\sum_{i=1}^nz_i$ over $\omega$ and $z$ while satisfying the constraints, we are replicating the loss function in problem (Sep 1). 

        Thus, the optimization problem (Sep 2) indeed solves the problem (Sep 1)

        \item [2.] Let us compute the Lagrangian of the problem (Sep 2). 
        
        Let $\lambda=(\lambda_1, \dots, \lambda_n), \pi\in\mathbb{R}^n $.
        \begin{equation}
            \begin{align}
                L(z, \omega, \lambda, \pi) &= \frac{1}{n\tau}\mathbf{1}^Tz+\frac{1}{2}||\omega||_2^2+\sum_{i=1}^n\lambda_i(1-y_i(\omega^Tx_i)-z_i)-\pi^Tz \\
                &=(\frac{1}{n\tau}\mathbf{1}-\lambda-\pi)^Tz+\frac{1}{2}||\omega||_2^2+\sum_{i=1}^n\lambda_i-\sum_{i=1}^n\lambda_iy_i(\omega^Tx_i)
            \end{align}
        \end{equation}

        To mininize $L$ w.r.t $z$ and $\omega$, both the coefficient of z  and the derivative of $L$ w.r.t $\omega$ need to be null.
        \begin{itemize}
            \item The coefficient of $z$ null gives the condition : $\frac{1}{n\tau}\mathbf{1}-\lambda-\pi = 0$.
            
            With $\pi \geq 0$ for the dual problem, this gives us $\lambda_i\leq\frac{1}{n\tau}$ for all $i\in \{1, \dots, n\}$.

            
            \item Let us compute the derivative of $L$ w.r.t $\omega$:

            $\frac{\partial L}{\partial \omega} = \omega-\sum_{i=1}^n\lambda_iy_ix_i=0 \Leftrightarrow \omega=\sum_{i=1}^n\lambda_iy_ix_i$
        \end{itemize}

        Thus, the Lagrange dual function is :
        \begin{equation}
            g(\lambda)=\frac{1}{2}\bigg|\bigg|\sum_{i=1}^n\lambda_iy_ix_i\bigg|\bigg|_2^2+\sum_{i=1}^n\lambda_i-\sum_{i=1}^n\sum_{j=1}^n\lambda_iy_i\lambda_jy_jx_j^Tx_i
        \end{equation}
        We can observe that :
        
        $\bigg|\bigg|\sum_{i=1}^n\lambda_iy_ix_i\bigg|\bigg|_2^2 = \bigg(\sum_{i=1}^n\lambda_iy_ix_i\bigg)^T\sum_{i=1}^n\lambda_iy_ix_i = \sum_{i=1}^n\sum_{j=1}^n\lambda_iy_i\lambda_jy_jx_j^Tx_i$

        Thus, $g(\lambda)=\sum_{i=1}^n\lambda_i-\frac{1}{2}\bigg|\bigg|\sum_{i=1}^n\lambda_iy_ix_i\bigg|\bigg|_2^2$

        To conclude, the dual problem of (Sep 2) is :
        \begin{equation}
            \begin{aligned}
                &\max_\lambda \quad \sum_{i=1}^n\lambda_i-\frac{1}{2}\bigg|\bigg|\sum_{i=1}^n\lambda_iy_ix_i\bigg|\bigg|_2^2 \\
                &\text{s.t.} \quad 0\leq\lambda_i\leq\frac{1}{n\tau}\quad \forall i=1,\dots, n
            \end{aligned}
        \end{equation}
    \end{itemize}
\end{document}
